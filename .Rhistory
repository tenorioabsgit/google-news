res_json
library(httr)
library(jsonlite)
# Função para obter dados de uma página específica
get_data <- function(page_number) {
headers <- c(
'X-API-KEY' = '0d6ecb3b03e5fa72316b21ac8fe3fad8a9e8a52d',
'Content-Type' = 'application/json'
)
body <- sprintf('{
"q": "LGPD OR ANPD",
"gl": "br",
"hl": "pt-br",
"num": 100,
"tbs": "cdr:1,cd_min:01/01/2018,cd_max:31/12/2023",
"page": %d
}', page_number)
res <- VERB("POST", url = "https://google.serper.dev/news", body = body, add_headers(.headers = headers))
res_text <- content(res, 'text')
res_json <- fromJSON(res_text)
return(as.data.frame(res_json))
}
# Obter dados de várias páginas e combinar em um único dataframe
all_data <- do.call(rbind, lapply(1:10, get_data)) # Altere o número 5 para quantas páginas você precisar
library(httr)
library(jsonlite)
# Função para obter dados de uma página específica
get_data <- function(page_number) {
headers <- c(
'X-API-KEY' = '0d6ecb3b03e5fa72316b21ac8fe3fad8a9e8a52d',
'Content-Type' = 'application/json'
)
body <- sprintf('{
"q": "LGPD OR ANPD",
"gl": "br",
"hl": "pt-br",
"num": 100,
"tbs": "cdr:1,cd_min:01/01/2018,cd_max:31/12/2023",
"page": %d
}', page_number)
res <- VERB("POST", url = "https://google.serper.dev/news", body = body, add_headers(.headers = headers))
res_text <- content(res, 'text')
res_json <- fromJSON(res_text)
return(as.data.frame(res_json))
}
# Obter dados de várias páginas e combinar em um único dataframe
all_data <- do.call(rbind, lapply(1:6, get_data)) # Altere o número 5 para quantas páginas você precisar
# Imprimir o dataframe resultante
print(all_data)
library(rvest)
library(tidyverse)
# Função para extrair dados de uma única página do Google News
extrair_noticias <- function(termo_busca, pagina = 1) {
# Construindo a URL da pesquisa
url <- paste0("https://www.google.com/search?q=", URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Lendo a página HTML
pagina_html <- read_html(url)
# Extraindo os dados relevantes (ajuste os seletores conforme a estrutura do Google News)
noticias <- pagina_html %>%
html_nodes(".XGn4Vd") %>%
html_element("a") %>%
html_attr("href") %>%
enframe(name = NULL, value = "link") %>%
mutate(
data_publicacao = pagina_html %>%
html_nodes(".wgL4Hf") %>%
html_text(),
publicador = pagina_html %>%
html_nodes(".y66b8d") %>%
html_text(),
conteudo = map(link, ~ read_html(.x) %>%
html_nodes("p") %>%
html_text() %>%
paste(collapse = " "))
)
return(noticias)
}
# Exemplo de uso
termo_busca <- "mudanças climáticas"
numero_paginas <- 3  # Número de páginas a serem extraídas
# Criando um dataframe para armazenar todas as notícias
todas_noticias <- data.frame()
# Loop para extrair dados de múltiplas páginas
for (i in 1:numero_paginas) {
noticias_pagina <- extrair_noticias(termo_busca, pagina = i)
todas_noticias <- bind_rows(todas_noticias, noticias_pagina)
}
# Visualizando os resultados
head(todas_noticias)
library(rvest)
library(tidyverse)
# Função para extrair dados de uma única página do Google News
extrair_noticias <- function(termo_busca, pagina = 1) {
# Construindo a URL da pesquisa
url <- paste0("https://www.google.com/search?q=", URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Lendo a página HTML
pagina_html <- read_html(url)
# Extraindo os dados relevantes (ajuste os seletores conforme a estrutura do Google News)
noticias <- pagina_html %>%
html_nodes(".new-article") %>%  # Replace with actual selector
html_element("a") %>%
html_attr("href") %>%
enframe(name = NULL, value = "link") %>%
mutate(
data_publicacao = pagina_html %>%
html_nodes(".publish-date") %>%  # Replace with actual selector
html_text(),
publicador = pagina_html %>%
html_nodes(".news-source") %>%  # Replace with actual selector
html_text(),
conteudo = map(link, ~ read_html(.x) %>%
html_nodes("p") %>%
html_text() %>%
paste(collapse = " "))
)
return(noticias)
}
# Exemplo de uso
termo_busca <- "mudanças climáticas"
numero_paginas <- 3  # Número de páginas a serem extraídas
# Criando um dataframe para armazenar todas as notícias
todas_noticias <- data.frame()
# Loop para extrair dados de múltiplas páginas
for (i in 1:numero_paginas) {
noticias_pagina <- extrair_noticias(termo_busca, pagina = i)
todas_noticias <- bind_rows(todas_noticias, noticias_pagina)
}
# Visualizando os resultados
head(todas_noticias)
library(rvest)
library(tidyverse)
# Função para extrair dados de uma única página do Google News
extrair_noticias <- function(termo_busca, pagina = 1) {
# Construindo a URL da pesquisa
url <- paste0("https://www.google.com/search?q=", URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Lendo a página HTML
pagina_html <- read_html(url)
# Extraindo os dados relevantes (ajuste os seletores conforme a estrutura do Google News)
noticias <- pagina_html %>%
html_nodes(".new-article") %>%  # Replace with actual selector
html_element("a") %>%
html_attr("href") %>%
enframe(name = NULL, value = "link") %>%
mutate(
data_publicacao = pagina_html %>%
html_nodes(".publish-date") %>%  # Replace with actual selector
html_text(),
publicador = pagina_html %>%
html_nodes(".news-source") %>%  # Replace with actual selector
html_text(),
conteudo = map(link, ~ read_html(.x) %>%
html_nodes("p") %>%
html_text() %>%
paste(collapse = " "))
)
return(noticias)
}
# Exemplo de uso
termo_busca <- "mudanças climáticas"
numero_paginas <- 3  # Número de páginas a serem extraídas
# Criando um dataframe para armazenar todas as notícias
todas_noticias <- data.frame()
# Loop para extrair dados de múltiplas páginas
for (i in 1:numero_paginas) {
noticias_pagina <- extrair_noticias(termo_busca, pagina = i)
todas_noticias <- bind_rows(todas_noticias, noticias_pagina)
}
# Visualizando os resultados
head(todas_noticias)
library(rvest)
library(tidyverse)
# Função para extrair dados de uma única página do Google News
extrair_noticias <- function(termo_busca, pagina = 1) {
# Construindo a URL da pesquisa
url <- paste0("https://www.google.com/search?q=", URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Lendo a página HTML
pagina_html <- read_html(url)
# Extraindo os dados relevantes (ajuste os seletores conforme a estrutura do Google News)
noticias <- pagina_html %>%
html_nodes(".g") %>%  # Exemplo de novo seletor para resultados de busca
html_element("a") %>%
html_attr("href") %>%
enframe(name = NULL, value = "link") %>%
mutate(
data_publicacao = pagina_html %>%
html_nodes(".l") %>%  # Exemplo de novo seletor para data
html_text(),
publicador = pagina_html %>%
html_nodes(".nDgy9d") %>%  # Exemplo de novo seletor para publicador
html_text(),
conteudo = map(link, ~ read_html(.x) %>%
html_nodes("p") %>%
html_text() %>%
paste(collapse = " "))
)
return(noticias)
}
# Exemplo de uso
termo_busca <- "mudanças climáticas"
numero_paginas <- 3  # Número de páginas a serem extraídas
# Criando um dataframe para armazenar todas as notícias
todas_noticias <- data.frame()
# Loop para extrair dados de múltiplas páginas
for (i in 1:numero_paginas) {
noticias_pagina <- extrair_noticias(termo_busca, pagina = i)
todas_noticias <- bind_rows(todas_noticias, noticias_pagina)
}
# Visualizando os resultados
head(todas_noticias)
library(rvest)
library(tidyverse)
library(RSelenium)
# Iniciar o servidor Selenium
rD <- rsDriver(browser = "chrome", port = 4545L)
library(rvest)
library(httr)
library(tibble)
# Função para extrair notícias do Google News
extrair_noticias_google_news <- function(termo_busca, pagina = 1) {
# Criar a URL de busca no Google News
url_base <- "https://www.google.com/search?q="
url <- paste0(url_base, URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Fazer a requisição HTTP
pagina <- GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"))
# Parsear o conteúdo HTML
pagina_html <- read_html(content(pagina, "text"))
# Extrair os títulos das notícias
titulos <- pagina_html %>%
html_nodes("h3") %>%
html_text()
# Extrair os links das notícias
links <- pagina_html %>%
html_nodes("h3 a") %>%
html_attr("href") %>%
# Limpar os links para remover a parte inicial do Google
gsub("/url\\?q=", "", .) %>%
gsub("&sa=.*", "", .)
# Extrair os trechos das notícias
trechos <- pagina_html %>%
html_nodes(".Y3v8qd") %>%
html_text()
# Extrair as fontes das notícias
fontes <- pagina_html %>%
html_nodes(".wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
html_text()
# Criar um dataframe com os dados extraídos
noticias_df <- tibble(
Titulo = titulos,
Link = links,
Fonte = fontes,
Trecho = trechos
)
return(noticias_df)
}
# Exemplo de uso
resultado <- extrair_noticias_google_news("tecnologia", pagina = 1)
library(rvest)
library(httr)
library(tibble)
# Função para extrair notícias do Google News
extrair_noticias_google_news <- function(termo_busca, pagina = 1) {
# Criar a URL de busca no Google News
url_base <- "https://www.google.com/search?q="
url <- paste0(url_base, URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Fazer a requisição HTTP
pagina <- GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"))
# Parsear o conteúdo HTML
pagina_html <- read_html(content(pagina, "text"))
# Extrair os títulos das notícias
titulos <- pagina_html %>%
html_nodes("h3") %>%
html_text()
# Extrair os links das notícias
links <- pagina_html %>%
html_nodes("a") %>% # Usar seletor para todos os links
html_attr("href") %>%
grep("^/url\\?q=", ., value = TRUE) %>% # Filtrar apenas os links que começam com '/url?q='
gsub("/url\\?q=", "", .) %>%
gsub("&sa=.*", "", .)
# Extrair os trechos das notícias
trechos <- pagina_html %>%
html_nodes(".Y3v8qd") %>%
html_text()
# Extrair as fontes das notícias
fontes <- pagina_html %>%
html_nodes(".wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
html_text()
# Criar um dataframe com os dados extraídos
noticias_df <- tibble(
Titulo = titulos,
Link = links[1:length(titulos)], # Garantir que o número de links corresponda ao número de títulos
Fonte = fontes[1:length(titulos)], # Garantir que o número de fontes corresponda ao número de títulos
Trecho = trechos[1:length(titulos)] # Garantir que o número de trechos corresponda ao número de títulos
)
return(noticias_df)
}
# Exemplo de uso
resultado <- extrair_noticias_google_news("tecnologia", pagina = 1)
# Visualizar os resultados
print(resultado)
View(resultado)
library(rvest)
library(httr)
library(tibble)
# Função para extrair notícias do Google News
extrair_noticias_google_news <- function(termo_busca, pagina = 1) {
# Criar a URL de busca no Google News
url_base <- "https://www.google.com/search?q="
url <- paste0(url_base, URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Fazer a requisição HTTP
pagina <- GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"))
# Parsear o conteúdo HTML
pagina_html <- read_html(content(pagina, "text"))
# Extrair os títulos das notícias
titulos <- pagina_html %>%
html_nodes("h3") %>%
html_text()
# Extrair os links das notícias
links <- pagina_html %>%
html_nodes("a") %>%
html_attr("href") %>%
grep("^/url\\?q=", ., value = TRUE) %>% # Filtrar apenas os links que começam com '/url?q='
gsub("/url\\?q=", "", .) %>%
gsub("&sa=.*", "", .)
# Extrair os trechos das notícias
trechos <- pagina_html %>%
html_nodes(".Y3v8qd") %>%
html_text()
# Extrair as fontes das notícias
fontes <- pagina_html %>%
html_nodes(".wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
html_text()
# Criar um dataframe com os dados extraídos
noticias_df <- tibble(
Titulo = titulos,
Link = if(length(links) > 0) links[1:length(titulos)] else NA,
Fonte = if(length(fontes) > 0) fontes[1:length(titulos)] else NA,
Trecho = if(length(trechos) > 0) trechos[1:length(titulos)] else NA
)
return(noticias_df)
}
# Exemplo de uso
resultado <- extrair_noticias_google_news("tecnologia", pagina = 1)
# Visualizar os resultados
print(resultado)
library(rvest)
library(httr)
library(tibble)
# Função para extrair notícias do Google News
extrair_noticias_google_news <- function(termo_busca, pagina = 1) {
# Criar a URL de busca no Google News
url_base <- "https://www.google.com/search?q="
url <- paste0(url_base, URLencode(termo_busca), "&tbm=nws&start=", (pagina - 1) * 10)
# Fazer a requisição HTTP
pagina <- GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"))
# Parsear o conteúdo HTML
pagina_html <- read_html(content(pagina, "text"))
# Extrair os títulos das notícias
titulos <- pagina_html %>%
html_nodes("h3") %>%
html_text()
# Verificar e extrair os links das notícias
links <- pagina_html %>%
html_nodes("a") %>%
html_attr("href") %>%
grep("^/url\\?q=", ., value = TRUE) %>% # Filtrar apenas os links que começam com '/url?q='
gsub("/url\\?q=", "", .) %>%
gsub("&sa=.*", "", .)
if (length(links) == 0) {
links <- rep(NA, length(titulos))
} else {
links <- links[1:length(titulos)]
}
# Verificar e extrair os trechos das notícias
trechos <- pagina_html %>%
html_nodes(".Y3v8qd") %>%
html_text()
if (length(trechos) == 0) {
trechos <- rep(NA, length(titulos))
} else {
trechos <- trechos[1:length(titulos)]
}
# Verificar e extrair as fontes das notícias
fontes <- pagina_html %>%
html_nodes(".wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
html_text()
if (length(fontes) == 0) {
fontes <- rep(NA, length(titulos))
} else {
fontes <- fontes[1:length(titulos)]
}
# Criar um dataframe com os dados extraídos
noticias_df <- tibble(
Titulo = titulos,
Link = links,
Fonte = fontes,
Trecho = trechos
)
return(noticias_df)
}
# Exemplo de uso
resultado <- extrair_noticias_google_news("tecnologia", pagina = 1)
# Visualizar os resultados
print(resultado)
library(RSelenium)
library(rvest)
library(tibble)
# Iniciar o servidor Selenium
rD <- rsDriver(browser = "chrome", port = 4545L, verbose = FALSE)
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
source("C:/Users/pira_abs/OneDrive/ciencia_de_dados/R/Google News/python_generico.R")
View(resultado)
# loading the packages:
library(dplyr) # for pipes and the data_frame function
library(rvest) # webscraping
library(stringr) # to deal with strings and to clean up our data
# extracting the whole website
google <- read_html("https://news.google.com/search?q=Atila%20Iamarino%20when%3A4y&hl=pt-BR&gl=BR&ceid=BR%3Apt-419")
article_all <- google %>% html_nodes("article")
times <- article_all %>%
html_node("time") %>%
html_text()
vehicles <- article_all %>%
html_nodes("a.wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
html_text()
headlines <- article_all %>%
html_nodes("a.DY5T1d") %>%
html_text()
tb_news <- tibble(headlines, vehicles, times)
news <- function(term) {
url <- paste0("https://news.google.com/search?q=", term, "&hl=es-419&gl=US&ceid=US:es-419")
nodeset <- read_html(url) %>% html_nodes("article")
dplyr::bind_rows(lapply(nodeset, function(x) tibble::tibble(
Title = x %>% html_node(".ipQwMb.ekueJc.RD0gLb") %>% html_text(),
Link = x %>% html_node(".ipQwMb.ekueJc.RD0gLb > a") %>% html_attr("href") %>% xml2::url_absolute(url),
Description = x %>% html_node("div.Da10Tb.Rai5ob > span") %>% html_text(),
Source = x %>% html_node("div.QmrVtf.RD0gLb.kybdz > div > a") %>% html_text(),
Time = x %>% html_node("div.QmrVtf.RD0gLb.kybdz > div > time") %>% html_attr("datetime")
)))
}
term <- news("coronavirus")
## função em portugues brasil
news <- function(term) {
url <- paste0("https://news.google.com/search?q=", term, "%20when%3A1y&hl=pt-BR&gl=BR&ceid=BR%3Apt-419")
nodeset <- read_html(url) %>% html_nodes("article")
dplyr::bind_rows(lapply(nodeset, function(x) tibble::tibble(
Title = x %>% html_node(".ipQwMb.ekueJc.RD0gLb") %>% html_text(),
Link = x %>% html_node(".ipQwMb.ekueJc.RD0gLb > a") %>% html_attr("href") %>% xml2::url_absolute(url),
Description = x %>% html_node("div.Da10Tb.Rai5ob > span") %>% html_text(),
Source = x %>% html_node("div.QmrVtf.RD0gLb.kybdz > div > a") %>% html_text(),
Time = x %>% html_node("div.QmrVtf.RD0gLb.kybdz > div > time") %>% html_attr("datetime")
)))
}
term <- news("coronavirus")
term
View(article_all)
View(google)
View(term)
library(httr)
library(jsonlite)
# Função para obter dados de uma página específica
get_data <- function(page_number) {
headers <- c(
'X-API-KEY' = '0d6ecb3b03e5fa72316b21ac8fe3fad8a9e8a52d',
'Content-Type' = 'application/json'
)
body <- sprintf('{
"q": "LGPD OR ANPD",
"gl": "br",
"hl": "pt-br",
"num": 100,
"tbs": "cdr:1,cd_min:01/01/2018,cd_max:31/12/2023",
"page": %d
}', page_number)
res <- VERB("POST", url = "https://google.serper.dev/news", body = body, add_headers(.headers = headers))
res_text <- content(res, 'text')
res_json <- fromJSON(res_text)
# Imprimir o número de resultados retornados nesta página
cat("Page:", page_number, "Number of results:", length(res_json$results), "\n")
return(as.data.frame(res_json$results))
}
# Inicializar a lista para armazenar todos os dados
all_data <- list()
# Inicializar a variável de controle da página
page_number <- 1
# Loop para continuar a buscar dados até que não haja mais resultados
while(TRUE) {
page_data <- get_data(page_number)
if (nrow(page_data) == 0) {
break
}
all_data[[page_number]] <- page_data
page_number <- page_number + 1
}
# Combinar todos os dataframes em um único dataframe
final_data <- do.call(rbind, all_data)
# Imprimir o dataframe resultante
print(final_data)
# Salvar o dataframe resultante em um arquivo CSV
write.csv(final_data, "resultados_LGPD_ANPD.csv", row.names = FALSE)
final_data
